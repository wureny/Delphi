---
name: "Story: Evaluation Benchmark"
about: "Define or implement Raw vs Ontology benchmark tasks"
title: "[Story][Evaluation] "
labels: ["type:story", "area:evaluation", "priority:p0"]
assignees: []
---

## Benchmark Goal
-

## Task Type
- [ ] E1 Task Set Design
- [ ] E2 Baseline (Raw Dataset Agent)
- [ ] E3 Comparison (Ontology Agent)
- [ ] E4 Report and Conclusion

## Eval Dataset Scope
- [ ]
- [ ]

## Metrics
- [ ] Retrieval accuracy
- [ ] Event understanding consistency
- [ ] Relation tracing success
- [ ] Conclusion consistency

## Acceptance Criteria
- [ ]
- [ ]

## Output Artifacts
- Eval config:
- Result table:
- Report path:

## References
- Parent Epic:
- PRD section:
- Related issues:
